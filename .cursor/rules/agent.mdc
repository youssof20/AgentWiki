---
description: Rules for building the self-improving agent and Agentwiki logic. Apply when editing agent.py, memory.py, evaluator.py.
globs: agent.py, memory.py, evaluator.py
alwaysApply: false
---

# AGENTWIKI LOOP (primary — from game-day-brief)
1. Agent receives task → Search platform for similar tasks (semantic or keyword).
2. Retrieve top N Method Cards → Compose plan using best methodology + "avoid these mistakes" + tool hints.
3. Execute task (LLM + tools).
4. Evaluate outcome (evaluator: rubric or LLM-as-judge, score).
5. Write back new or updated Method Card (what worked, what failed, fixes, outcome score).
Demo must support **side-by-side**: Run 1 (static agent, no library) vs Run 2 (with Agentwiki) — show time, retries, quality score, errors.

# METHOD CARD SCHEMA (agent-readable playbook — strict JSON)
Every Method Card MUST have this structure. Store in ClickHouse when available; fallback Supabase or local JSON.
{
  "id": "uuid",
  "timestamp": "ISO 8601",
  "task_intent": "string (what was asked)",
  "context": "string (constraints, tools available)",
  "plan": "string (step-by-step methodology)",
  "tool_calls": "string or list (key actions)",
  "mistakes": "string (failure points)",
  "fixes": "string (lessons learned)",
  "outcome_score": "float or int",
  "tags": ["task_type", "industry", "tools"]
}
Optional: user_input, agent_output, rating, quality_label, generation for backward compatibility with simple rating loop.

# LEGACY / SIMPLE SELF-IMPROVEMENT (if not using Method Cards yet)
Loop: User Input → LLM Call → Output → User Rates (1-5) → Store to Memory → Improves Next System Prompt.
Memory entry: id, timestamp, user_input, agent_output, rating, quality_label, generation, system_prompt_used.

# SYSTEM PROMPT SELF-IMPROVEMENT PATTERN
**For Agentwiki:** Include retrieved Method Cards in the prompt: best plan/methodology, "avoid these mistakes" section, tool hints. Agent composes improved plan from top N cards.
**For simple loop:** When building the system prompt: (1) Base task instruction. (2) Read last 5 "good" memory entries (rating >= 4). (3) Extract what made them good. (4) Append "Based on past successful responses, you know that the user prefers: [extracted patterns]". Never skip the improvement step.

# LLM CALL HIERARCHY (try in this order — four independent free providers)
1. **Groq** (primary — fastest, 14.4k req/day free) — GROQ_API_KEY
2. **OpenRouter** (fallback 1 — 30+ free models) — OPENROUTER_API_KEY
3. **Mistral** (fallback 2 — 1B tokens/month free) — MISTRAL_API_KEY
4. **Gemini** (last resort — finicky free tier) — GEMINI_API_KEY

# GROQ API PATTERN (primary)
```python
from groq import Groq
client = Groq(api_key=os.getenv("GROQ_API_KEY"))
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
)
return response.choices[0].message.content
```

# OPENROUTER / MISTRAL (OpenAI-compatible — same code, swap base_url + model)
```python
from openai import OpenAI

def chat_openai_compatible(api_key: str, base_url: str, model: str, system_prompt: str, user_input: str) -> str:
    client = OpenAI(api_key=api_key, base_url=base_url)
    r = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]
    )
    return r.choices[0].message.content or ""

# OpenRouter: base_url="https://openrouter.ai/api/v1", model e.g. "meta-llama/llama-3.3-70b-instruct"
# Mistral:   base_url="https://api.mistral.ai/v1",     model e.g. "mistral-small" or "open-mistral-7b"
```

# GEMINI API PATTERN (last resort only)
```python
import google.generativeai as genai
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel("gemini-2.0-flash", system_instruction=system_prompt)
response = model.generate_content(user_input)
return response.text
```

# IMPROVEMENT METRICS TO ALWAYS EXPOSE
- For Agentwiki: search_count, retrieve_top_n, run_score (Run 1 vs Run 2), time_to_completion, retry_count.
- For simple loop: get_agent_generation(), get_average_score(), get_score_trend(), get_learned_patterns().

# NEVER DO THIS
- Don't call the LLM more than once per user action (costs and speed).
- Don't store raw API response objects — only store the .text / .content string.
- Don't let memory or Method Card store grow unbounded — cap (e.g. last 50–100 entries).
